{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Length.\n",
    "n = 100\n",
    "STRIDE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(data, fs):\n",
    "    f1 = np.mean(data, axis=0)\n",
    "    C = np.cov(data.T)\n",
    "    f2 = np.concatenate((C[0, 0:3], C[1, 1:3], C[2, 2:3]))\n",
    "    f3 = np.array([skew(data.iloc[:, 0]), skew(data.iloc[:, 1]), skew(data.iloc[:, 2])])\n",
    "    f4 = np.array([kurtosis(data.iloc[:, 0]), kurtosis(data.iloc[:, 1]), kurtosis(data.iloc[:, 2])])\n",
    "    f5 = np.zeros(3)\n",
    "    f6 = np.zeros(3)\n",
    "    for i in range(0,3):\n",
    "        g = abs(np.fft.fft(data.iloc[:,i]))\n",
    "        g = g[0:round(len(g)/2)]\n",
    "        g[0] = 0\n",
    "        max_i = np.argmax(g)\n",
    "        f5[i] = g[max_i]\n",
    "        f6[i] = fs * max_i\n",
    "    return np.concatenate((f1, f2, f3, f4, f5, f6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df, stride=STRIDE):\n",
    "    df_out = pd.DataFrame(columns=[i for i in range(0, 42)])\n",
    "    fs = 1 / n\n",
    "    i = 0\n",
    "    for i in range(0, df.shape[0] - n, stride):\n",
    "        features_accel = getFeatures(df.iloc[i:i+n, 0:3], fs)\n",
    "        features_gyro = getFeatures(df.iloc[i:i+n, 3:6], fs)\n",
    "        features = np.concatenate((features_accel, features_gyro))\n",
    "        df_out.loc[i] = features\n",
    "        i += 1\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    df_out = pd.DataFrame(columns=[i for i in range(0, 42)])\n",
    "\n",
    "    features_accel = df.iloc[:, 0:3]\n",
    "    features_gyro = df.iloc[:, 3:6]\n",
    "    features = np.concatenate((features_accel, features_gyro))\n",
    "    df_out = pd.DataFrame(features)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose via MODE ## TBD ##\n",
    "def get_detection_window(df, stride=STRIDE):\n",
    "    df_out = pd.DataFrame(columns=[0])\n",
    "    i = 0\n",
    "    for i in range(0, df.shape[0] - n, stride):\n",
    "        df_slice = df.iloc[i:i+n,:]\n",
    "        df_out.loc[i] = df_slice.mode().iloc[0,0]\n",
    "#         df_out.loc[i] = df.iloc[i+n-1]\n",
    "        i += 1\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_predictions(predictions, size):\n",
    "    extrapolated = np.fromiter((val for val in predictions for _ in range(0, STRIDE)), float)\n",
    "    return extrapolated[0:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_data(session):\n",
    "    df_data_arm = pd.read_csv('test_data/{}/armIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    size = df_data_arm.shape[0]\n",
    "    arm_features = generate_features(df_data_arm, stride=1)\n",
    "    df_data_wrist = pd.read_csv('test_data/{}/wristIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    wrist_features = generate_features(df_data_wrist, stride=1)\n",
    "    zero_padding = pd.DataFrame(np.zeros(n))\n",
    "    return size, pd.concat([arm_features, wrist_features], axis=1, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2DTranspose, Dropout, Activation, BatchNormalization, UpSampling2D, Concatenate\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, concatenate, TimeDistributed, Flatten, LSTM, Dense, GlobalAveragePooling1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training Session01\n",
      "Processing training Session05\n",
      "Processing training Session06\n",
      "Processing training Session07\n",
      "Processing training Session12\n",
      "Saved training features!\n",
      "Processing validation Session13\n",
      "Saved validation features!\n"
     ]
    }
   ],
   "source": [
    "# Initializing dataframes for training and validation.\n",
    "x_train = pd.DataFrame(columns=[i for i in range(0, 84)])\n",
    "y_train = pd.DataFrame(columns=[0])\n",
    "x_val = pd.DataFrame(columns=[i for i in range(0, 84)])\n",
    "y_val = pd.DataFrame(columns=[0])\n",
    "\n",
    "train_sessions = ['Session01', 'Session05', 'Session06', 'Session07', 'Session12']\n",
    "# sessions = ['Session01_tmp']\n",
    "\n",
    "for session in train_sessions:\n",
    "    print('Processing training {}'.format(session))\n",
    "    df_data_arm = pd.read_csv('TrainingData/{}/armIMU.txt'.format(session),  delim_whitespace=True, header=None)\n",
    "    arm_features = generate_features(df_data_arm)\n",
    "    df_data_wrist = pd.read_csv('TrainingData/{}/wristIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    wrist_features = generate_features(df_data_wrist)\n",
    "    features = pd.concat([arm_features, wrist_features], axis=1, sort=False, ignore_index=True)\n",
    "    df_data_detection = pd.read_csv('TrainingData/{}/detection.txt'.format(session), header=None)\n",
    "    x_train = x_train.append(other=features, ignore_index=True)\n",
    "    y_train = y_train.append(other=df_data_detection, ignore_index=True)\n",
    "#     y_train = y_train.append(other=get_detection_window(df_data_detection), ignore_index=True)\n",
    "\n",
    "x_train.to_csv('x_train.txt', header=False, index=False, sep='\\t', mode='a')\n",
    "y_train.to_csv('y_train.txt', header=False, index=False, sep='\\t', mode='a')\n",
    "print('Saved training features!')\n",
    "    \n",
    "val_sessions = ['Session13']\n",
    "for session in val_sessions:\n",
    "    print('Processing validation {}'.format(session))\n",
    "    df_data_arm = pd.read_csv('TrainingData/{}/armIMU.txt'.format(session),  delim_whitespace=True, header=None)\n",
    "    arm_features = generate_features(df_data_arm)\n",
    "    df_data_wrist = pd.read_csv('TrainingData/{}/wristIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    wrist_features = generate_features(df_data_wrist)\n",
    "    features = pd.concat([arm_features, wrist_features], axis=1, sort=False, ignore_index=True)\n",
    "    df_data_detection = pd.read_csv('TrainingData/{}/detection.txt'.format(session), header=None)\n",
    "    x_val = x_val.append(other=features, ignore_index=True)\n",
    "    y_val = y_val.append(other=df_data_detection, ignore_index=True)\n",
    "#     y_train = y_train.append(other=get_detection_window(df_data_detection), ignore_index=True)\n",
    "    \n",
    "x_val.to_csv('x_val.txt', header=False, index=False, sep='\\t', mode='a')\n",
    "y_val.to_csv('y_val.txt', header=False, index=False, sep='\\t', mode='a')\n",
    "print('Saved validation features!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_df = pd.read_csv('x_train.txt', sep='\\t')\n",
    "# y_train_df = pd.read_csv('y_train.txt', sep='\\t')\n",
    "x_train_df = np.loadtxt('x_train.txt', delimiter='\\t')\n",
    "y_train_df = np.loadtxt('y_train.txt', delimiter='\\t')\n",
    "\n",
    "X_train_reshaped = np.reshape(x_train_df, (x_train_df.shape[0], 1, x_train_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_val_df = pd.read_csv('x_val.txt', sep='\\t')\n",
    "# y_val_df = pd.read_csv('y_val.txt', sep='\\t')\n",
    "x_val_df = np.loadtxt('x_val.txt', delimiter='\\t')\n",
    "y_val_df = np.loadtxt('y_val.txt', delimiter='\\t')\n",
    "\n",
    "X_val_reshaped = np.reshape(x_val_df, (x_val_df.shape[0], 1, x_val_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val_reshaped.shape)\n",
    "print(y_val_df.shape)\n",
    "\n",
    "print(X_train_reshaped.shape)\n",
    "print(y_train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = 84\n",
    "timesteps = 1\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True,\n",
    "               batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "model.add(LSTM(32, return_sequences=True, stateful=True))\n",
    "model.add(LSTM(32, stateful=True))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_reshaped, y_train_df,\n",
    "          batch_size=1, epochs=10,\n",
    "          validation_data=(X_val_reshaped, y_val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_val_reshaped, y_val_df, batch_size=1)\n",
    "print(list(zip(model.metrics_names,score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

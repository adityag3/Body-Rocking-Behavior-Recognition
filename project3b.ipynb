{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Length.\n",
    "n = 100\n",
    "STRIDE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(data, fs):\n",
    "    f1 = np.mean(data, axis=0)\n",
    "    C = np.cov(data.T)\n",
    "    f2 = np.concatenate((C[0, 0:3], C[1, 1:3], C[2, 2:3]))\n",
    "    f3 = np.array([skew(data.iloc[:, 0]), skew(data.iloc[:, 1]), skew(data.iloc[:, 2])])\n",
    "    f4 = np.array([kurtosis(data.iloc[:, 0]), kurtosis(data.iloc[:, 1]), kurtosis(data.iloc[:, 2])])\n",
    "    f5 = np.zeros(3)\n",
    "    f6 = np.zeros(3)\n",
    "    for i in range(0,3):\n",
    "        g = abs(np.fft.fft(data.iloc[:,i]))\n",
    "        g = g[0:round(len(g)/2)]\n",
    "        g[0] = 0\n",
    "        max_i = np.argmax(g)\n",
    "        f5[i] = g[max_i]\n",
    "        f6[i] = fs * max_i\n",
    "    return np.concatenate((f1, f2, f3, f4, f5, f6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df, stride=STRIDE):\n",
    "    df_out = pd.DataFrame(columns=[i for i in range(0, 42)])\n",
    "    fs = 1 / n\n",
    "    i = 0\n",
    "    for i in range(0, df.shape[0] - n, stride):\n",
    "        features_accel = getFeatures(df.iloc[i:i+n, 0:3], fs)\n",
    "        features_gyro = getFeatures(df.iloc[i:i+n, 3:6], fs)\n",
    "        features = np.concatenate((features_accel, features_gyro))\n",
    "        df_out.loc[i] = features\n",
    "        i += 1\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose via MODE ## TBD ##\n",
    "def get_detection_window(df, stride=STRIDE):\n",
    "    df_out = pd.DataFrame(columns=[0])\n",
    "    i = 0\n",
    "    for i in range(0, df.shape[0] - n, stride):\n",
    "        df_slice = df.iloc[i:i+n,:]\n",
    "        df_out.loc[i] = df_slice.mode().iloc[0,0]\n",
    "#         df_out.loc[i] = df.iloc[i+n-1]\n",
    "        i += 1\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_predictions(predictions, size):\n",
    "    extrapolated = np.fromiter((val for val in predictions for _ in range(0, STRIDE)), float)\n",
    "    return extrapolated[0:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_data(session):\n",
    "    df_data_arm = pd.read_csv('test_data/{}/armIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    size = df_data_arm.shape[0]\n",
    "    arm_features = generate_features(df_data_arm, stride=1)\n",
    "    df_data_wrist = pd.read_csv('test_data/{}/wristIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    wrist_features = generate_features(df_data_wrist, stride=1)\n",
    "    zero_padding = pd.DataFrame(np.zeros(n))\n",
    "    return size, pd.concat([arm_features, wrist_features], axis=1, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2DTranspose, Dropout, Activation, BatchNormalization, UpSampling2D, Concatenate\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, concatenate, TimeDistributed, Flatten, LSTM, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model.\n",
    "filters = 2\n",
    "kernel_size = 2\n",
    "model = Sequential()\n",
    "# define CNN model\n",
    "model.add(TimeDistributed(Conv1D(filters,kernel_size)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last')))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "# define LSTM model\n",
    "model.add(LSTM(units=3, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False))\n",
    "model.add(Dense(units=3, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', loss='mse', metrics=['mae', 'mape', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training Session01\n",
      "Processing training Session05\n",
      "Processing training Session06\n",
      "Processing training Session07\n",
      "Processing training Session12\n",
      "Saved training features!\n",
      "Processing validation Session13\n",
      "Saved validation features!\n"
     ]
    }
   ],
   "source": [
    "# Initializing dataframes for training and validation.\n",
    "x_train = pd.DataFrame(columns=[i for i in range(0, 84)])\n",
    "y_train = pd.DataFrame(columns=[0])\n",
    "x_val = pd.DataFrame(columns=[i for i in range(0, 84)])\n",
    "y_val = pd.DataFrame(columns=[0])\n",
    "\n",
    "train_sessions = ['Session01', 'Session05', 'Session06', 'Session07', 'Session12']\n",
    "# sessions = ['Session01_tmp']\n",
    "\n",
    "for session in train_sessions:\n",
    "    print('Processing training {}'.format(session))\n",
    "    df_data_arm = pd.read_csv('TrainingData/{}/armIMU.txt'.format(session),  delim_whitespace=True, header=None)\n",
    "    arm_features = generate_features(df_data_arm)\n",
    "    df_data_wrist = pd.read_csv('TrainingData/{}/wristIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    wrist_features = generate_features(df_data_wrist)\n",
    "    features = pd.concat([arm_features, wrist_features], axis=1, sort=False, ignore_index=True)\n",
    "    df_data_detection = pd.read_csv('TrainingData/{}/detection.txt'.format(session), header=None)\n",
    "    x_train = x_train.append(other=features, ignore_index=True)\n",
    "    y_train = y_train.append(other=get_detection_window(df_data_detection), ignore_index=True)\n",
    "\n",
    "x_train.to_csv('x_train.txt', header=False, index=False, sep='\\t', mode='a')\n",
    "y_train.to_csv('y_train.txt', header=False, index=False, sep='\\t', mode='a')\n",
    "print('Saved training features!')\n",
    "    \n",
    "val_sessions = ['Session13']\n",
    "for session in val_sessions:\n",
    "    print('Processing validation {}'.format(session))\n",
    "    df_data_arm = pd.read_csv('TrainingData/{}/armIMU.txt'.format(session),  delim_whitespace=True, header=None)\n",
    "    arm_features = generate_features(df_data_arm)\n",
    "    df_data_wrist = pd.read_csv('TrainingData/{}/wristIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    wrist_features = generate_features(df_data_wrist)\n",
    "    features = pd.concat([arm_features, wrist_features], axis=1, sort=False, ignore_index=True)\n",
    "    df_data_detection = pd.read_csv('TrainingData/{}/detection.txt'.format(session), header=None)\n",
    "    x_val = x_val.append(other=features, ignore_index=True)\n",
    "    y_val = y_val.append(other=get_detection_window(df_data_detection), ignore_index=True)\n",
    "    \n",
    "x_val.to_csv('x_val.txt', header=False, index=False, sep='\\t', mode='a')\n",
    "y_val.to_csv('y_val.txt', header=False, index=False, sep='\\t', mode='a')\n",
    "print('Saved validation features!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_df = pd.read_csv('x_train.txt', sep='\\t')\n",
    "# y_train_df = pd.read_csv('y_train.txt', sep='\\t')\n",
    "x_train_df = np.loadtxt('x_train.txt', delimiter='\\t')\n",
    "y_train_df = np.loadtxt('y_train.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_val_df = pd.read_csv('x_val.txt', sep='\\t')\n",
    "# y_val_df = pd.read_csv('y_val.txt', sep='\\t')\n",
    "x_val_df = np.loadtxt('x_val.txt', delimiter='\\t')\n",
    "y_val_df = np.loadtxt('y_val.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 84)\n",
      "(371,)\n",
      "(18115, 84)\n",
      "(18115,)\n"
     ]
    }
   ],
   "source": [
    "print(x_val_df.shape)\n",
    "print(y_val_df.shape)\n",
    "\n",
    "print(x_train_df.shape)\n",
    "print(y_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute '_feed_input_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e78995c137b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    741\u001b[0m             \u001b[0;31m# Case: symbolic-mode subclassed network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0;31m# Do not do shape validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m             \u001b[0mfeed_input_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute '_feed_input_names'"
     ]
    }
   ],
   "source": [
    "model.fit(x=x_train_df, y=y_train_df, epochs=5, verbose=1, callbacks=None, validation_split=0.0, validation_data=(x_val_df, y_val_df), shuffle=False, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

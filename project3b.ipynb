{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-89894019024c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mto_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_graph' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "to_graph(Sequential())\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Length.\n",
    "n = 100\n",
    "STRIDE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(data, fs):\n",
    "    f1 = np.mean(data, axis=0)\n",
    "    C = np.cov(data.T)\n",
    "    f2 = np.concatenate((C[0, 0:3], C[1, 1:3], C[2, 2:3]))\n",
    "    f3 = np.array([skew(data.iloc[:, 0]), skew(data.iloc[:, 1]), skew(data.iloc[:, 2])])\n",
    "    f4 = np.array([kurtosis(data.iloc[:, 0]), kurtosis(data.iloc[:, 1]), kurtosis(data.iloc[:, 2])])\n",
    "    f5 = np.zeros(3)\n",
    "    f6 = np.zeros(3)\n",
    "    for i in range(0,3):\n",
    "        g = abs(np.fft.fft(data.iloc[:,i]))\n",
    "        g = g[0:round(len(g)/2)]\n",
    "        g[0] = 0\n",
    "        max_i = np.argmax(g)\n",
    "        f5[i] = g[max_i]\n",
    "        f6[i] = fs * max_i\n",
    "    return np.concatenate((f1, f2, f3, f4, f5, f6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df, stride=STRIDE):\n",
    "    df_out = pd.DataFrame(columns=[i for i in range(0, 42)])\n",
    "    fs = 1 / n\n",
    "    i = 0\n",
    "    for i in range(0, df.shape[0] - n, stride):\n",
    "        features_accel = getFeatures(df.iloc[i:i+n, 0:3], fs)\n",
    "        features_gyro = getFeatures(df.iloc[i:i+n, 3:6], fs)\n",
    "        features = np.concatenate((features_accel, features_gyro))\n",
    "        df_out.loc[i] = features\n",
    "        i += 1\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose via MODE ## TBD ##\n",
    "def get_detection_window(df, stride=STRIDE):\n",
    "    df_out = pd.DataFrame(columns=[0])\n",
    "    i = 0\n",
    "    for i in range(0, df.shape[0] - n, stride):\n",
    "        df_slice = df.iloc[i:i+n,:]\n",
    "        df_out.loc[i] = df_slice.mode().iloc[0,0]\n",
    "#         df_out.loc[i] = df.iloc[i+n-1]\n",
    "        i += 1\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_predictions(predictions, size):\n",
    "    extrapolated = np.fromiter((val for val in predictions for _ in range(0, STRIDE)), float)\n",
    "    return extrapolated[0:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_data(session):\n",
    "    df_data_arm = pd.read_csv('test_data/{}/armIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    print('debug_read_data_1')\n",
    "    size = df_data_arm.shape[0]\n",
    "    print('debug_read_data_2')\n",
    "    arm_features = generate_features(df_data_arm, stride=1)\n",
    "    print('debug_read_data_3')\n",
    "    df_data_wrist = pd.read_csv('test_data/{}/wristIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    print('debug_read_data_4')\n",
    "    wrist_features = generate_features(df_data_wrist, stride=1)\n",
    "    print('debug_read_data_5')\n",
    "#     df_data_detection = pd.read_csv('test_data/{}/detection.txt', header=None)\n",
    "#     df_data_detection = get_detection_window(df_data_detection, stride=1)\n",
    "    zero_padding = pd.DataFrame(np.zeros(n))\n",
    "    print('debug_read_data_6')\n",
    "    return size, pd.concat([arm_features, wrist_features], axis=1, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2DTranspose, Dropout, Activation, BatchNormalization, UpSampling2D, Concatenate\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, concatenate, TimeDistributed\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_block(input_tensor, n_filters, kernel_size):\n",
    "    # first layer\n",
    "    # Create a Conv2D layer with n_filters and a kernel of dimension : kernel_size x kernel_size. \n",
    "    # Use same padding and he_normal initializer\n",
    "    # YOUR CODE HERE\n",
    "#     x = Conv2D(n_filters, (kernel_size, kernel_size), strides=(1, 1), padding='same', data_format=None, dilation_rate=(1, 1),\n",
    "#         activation=None, use_bias=True, kernel_initializer='he_normal', bias_initializer='zeros', kernel_regularizer=None,\n",
    "#         bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)(input_tensor)\n",
    "    x = Conv2D(n_filters, (kernel_size, kernel_size), kernel_initializer='he_normal', padding=\"same\")(input_tensor)\n",
    "\n",
    "    # add a BatchNormalization layer\n",
    "    # YOUR CODE HERE\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "        \n",
    "    # Add a relu non-linearity (keras.layers.Activation)\n",
    "    # YOUR CODE HERE\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # second layer\n",
    "    # repeat the above steps (Conv + batchnorm + relu) taking the output of relu layer as input for this convolutional layer\n",
    "    # YOUR CODE HERE\n",
    "    x = Conv2D(n_filters, kernel_size = (kernel_size, kernel_size), strides=(1, 1), padding = 'same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # return the output tensor\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-83f9246982c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# define CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# define CNN model\n",
    "model.add(TimeDistributed(Conv1D(filters,kernel_size)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2, strides=None, padding='valid', data_format='channels_last')))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "# define LSTM model\n",
    "model.add(LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False))\n",
    "model.add(Dense(units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

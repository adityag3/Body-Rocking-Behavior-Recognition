{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from scipy.stats import kurtosis, skew\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2DTranspose, Dropout, Activation, BatchNormalization, UpSampling2D, Concatenate\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, concatenate, TimeDistributed, Flatten, LSTM, Dense, GlobalAveragePooling1D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Length.\n",
    "n = 100\n",
    "STRIDE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(data, fs):\n",
    "    f1 = np.mean(data, axis=0)\n",
    "    C = np.cov(data.T)\n",
    "    f2 = np.concatenate((C[0, 0:3], C[1, 1:3], C[2, 2:3]))\n",
    "    f3 = np.array([skew(data.iloc[:, 0]), skew(data.iloc[:, 1]), skew(data.iloc[:, 2])])\n",
    "    f4 = np.array([kurtosis(data.iloc[:, 0]), kurtosis(data.iloc[:, 1]), kurtosis(data.iloc[:, 2])])\n",
    "    f5 = np.zeros(3)\n",
    "    f6 = np.zeros(3)\n",
    "    for i in range(0,3):\n",
    "        g = abs(np.fft.fft(data.iloc[:,i]))\n",
    "        g = g[0:round(len(g)/2)]\n",
    "        g[0] = 0\n",
    "        max_i = np.argmax(g)\n",
    "        f5[i] = g[max_i]\n",
    "        f6[i] = fs * max_i\n",
    "    return np.concatenate((f1, f2, f3, f4, f5, f6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df, stride=STRIDE):\n",
    "    df_out = pd.DataFrame(columns=[i for i in range(0, 42)])\n",
    "    fs = 1 / n\n",
    "    i = 0\n",
    "    for i in range(0, df.shape[0] - n, stride):\n",
    "        features_accel = getFeatures(df.iloc[i:i+n, 0:3], fs)\n",
    "        features_gyro = getFeatures(df.iloc[i:i+n, 3:6], fs)\n",
    "        features = np.concatenate((features_accel, features_gyro))\n",
    "        df_out.loc[i] = features\n",
    "        i += 1\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose via MODE ## TBD ##\n",
    "def get_detection_window(df, stride=STRIDE):\n",
    "    df_out = pd.DataFrame(columns=[0])\n",
    "    i = 0\n",
    "    for i in range(0, df.shape[0] - n, stride):\n",
    "        df_slice = df.iloc[i:i+n,:]\n",
    "        df_out.loc[i] = df_slice.mode().iloc[0,0]\n",
    "#         df_out.loc[i] = df.iloc[i+n-1]\n",
    "        i += 1\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_predictions(predictions, size):\n",
    "    extrapolated = np.fromiter((val for val in predictions for _ in range(0, STRIDE)), float)\n",
    "    return extrapolated[0:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training Session01\n",
      "Processing training Session05\n",
      "Processing training Session06\n",
      "Processing training Session07\n",
      "Processing training Session12\n",
      "906113\n",
      "18115\n",
      "Saved training features!\n",
      "Saved validation features!\n"
     ]
    }
   ],
   "source": [
    "# Initializing dataframes for training and validation.\n",
    "x_train = pd.DataFrame(columns=[i for i in range(0, 12)])\n",
    "y_train = pd.DataFrame(columns=[0])\n",
    "x_val = pd.DataFrame(columns=[i for i in range(0, 12)])\n",
    "y_val = pd.DataFrame(columns=[0])\n",
    "\n",
    "train_sessions = ['Session01', 'Session05', 'Session06', 'Session07', 'Session12']\n",
    "# sessions = ['Session01_tmp']\n",
    "count = 0\n",
    "count_res = 0\n",
    "for session in train_sessions:\n",
    "    print('Processing training {}'.format(session))\n",
    "    df_data_arm = pd.read_csv('TrainingData/{}/armIMU.txt'.format(session),  delim_whitespace=True, header=None)\n",
    "    arm_features = generate_features(df_data_arm)\n",
    "    df_data_wrist = pd.read_csv('TrainingData/{}/wristIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    wrist_features = generate_features(df_data_wrist)\n",
    "    count += df_data_arm.shape[0]\n",
    "    features = pd.concat([arm_features, wrist_features], axis=1, sort=False, ignore_index=True)\n",
    "    count_res += features.shape[0]\n",
    "    df_data_detection = pd.read_csv('TrainingData/{}/detection.txt'.format(session), header=None)\n",
    "    x_train = x_train.append(other=features, ignore_index=True)\n",
    "    y_train = y_train.append(other=get_detection_window(df_data_detection), ignore_index=True)\n",
    "print(count)\n",
    "print(count_res)\n",
    "x_train.to_csv('x_train.txt', header=False, index=False, sep='\\t')\n",
    "y_train.to_csv('y_train.txt', header=False, index=False, sep='\\t')\n",
    "print('Saved training features!')\n",
    "    \n",
    "val_sessions = ['Session13']\n",
    "for session in val_sessions:\n",
    "    df_data_arm = pd.read_csv('TrainingData/{}/armIMU.txt'.format(session),  delim_whitespace=True, header=None)\n",
    "    arm_features = generate_features(df_data_arm)\n",
    "    df_data_wrist = pd.read_csv('TrainingData/{}/wristIMU.txt'.format(session), delim_whitespace=True, header=None)\n",
    "    wrist_features = generate_features(df_data_wrist)\n",
    "    count += df_data_arm.shape[0]\n",
    "    features = pd.concat([arm_features, wrist_features], axis=1, sort=False, ignore_index=True)\n",
    "    count_res += features.shape[0]\n",
    "    df_data_detection = pd.read_csv('TrainingData/{}/detection.txt'.format(session), header=None)\n",
    "    x_val = x_val.append(other=features, ignore_index=True)\n",
    "    y_val = y_val.append(other=get_detection_window(df_data_detection), ignore_index=True)\n",
    "    \n",
    "x_val.to_csv('x_val.txt', header=False, index=False, sep='\\t')\n",
    "y_val.to_csv('y_val.txt', header=False, index=False, sep='\\t')\n",
    "print('Saved validation features!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = np.loadtxt('x_train.txt', delimiter='\\t')\n",
    "y_train_df = np.loadtxt('y_train.txt', delimiter='\\t')\n",
    "x_val_df = np.loadtxt('x_val.txt', delimiter='\\t')\n",
    "y_val_df = np.loadtxt('y_val.txt', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_zeroes = np.zeros((31, 12))\n",
    "# y_zeroes = np.zeros((31,))\n",
    "# x_train_df = np.concatenate((x_train_df, x_zeroes), axis=0)\n",
    "# y_train_df = np.concatenate((y_train_df, y_zeroes), axis=0)\n",
    "X_train_reshaped = np.reshape(x_train_df, (x_train_df.shape[0], 1, x_train_df.shape[1]))\n",
    "\n",
    "# x_val_zeroes = np.zeros((10, 12))\n",
    "# y_val_zeroes = np.zeros((10,))\n",
    "# x_val_df = np.concatenate((x_val_df, x_val_zeroes), axis=0)\n",
    "# y_val_df = np.concatenate((y_val_df, y_val_zeroes), axis=0)\n",
    "X_val_reshaped = np.reshape(x_val_df, (x_val_df.shape[0], 1, x_val_df.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 1, 84)\n",
      "(371,)\n",
      "(18115, 1, 84)\n",
      "(18115,)\n"
     ]
    }
   ],
   "source": [
    "print(X_val_reshaped.shape)\n",
    "print(y_val_df.shape)\n",
    "\n",
    "print(X_train_reshaped.shape)\n",
    "print(y_train_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = 84\n",
    "timesteps = 1\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(nb_filter=20, filter_length=1, padding='same', input_shape=(1, 84)))\n",
    "model.add(LSTM(32, batch_input_shape=(batch_size, timesteps, data_dim)))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 18115 samples, validate on 371 samples\n",
      "Epoch 1/10\n",
      "18115/18115 [==============================] - 137s 8ms/step - loss: 0.3983 - acc: 0.8319 - val_loss: 0.5865 - val_acc: 0.6954\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.39829, saving model to model.h5\n",
      "Epoch 2/10\n",
      "18115/18115 [==============================] - 133s 7ms/step - loss: 0.3598 - acc: 0.8538 - val_loss: 0.4727 - val_acc: 0.7116\n",
      "\n",
      "Epoch 00002: loss improved from 0.39829 to 0.35980, saving model to model.h5\n",
      "Epoch 3/10\n",
      "13555/18115 [=====================>........] - ETA: 33s - loss: 0.3570 - acc: 0.8570"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_reshaped, y_train_df, batch_size=1, epochs=10, validation_data=(X_val_reshaped, y_val_df), callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_val_reshaped, y_val_df, batch_size=1)\n",
    "print(list(zip(model.metrics_names,score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
